{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNqBvcm8_Ndb"
   },
   "source": [
    "# Multi-Language Speech Recognition and Speaker Diarisation\n",
    "\n",
    "This demo allows you to recognize speech in 99 different languages, identify speakers, and translate the text into a selected language. The pipeline is made up of three libraries: \n",
    "* [denoiser](https://github.com/facebookresearch/denoiser) to remove extraneous noise from the audio,\n",
    "* [pyannote](https://github.com/pyannote/pyannote-audio) for speaker diarisation, and\n",
    "* [whisper](https://github.com/openai/whisper) - the main component that not only recognizes speech but also has the ability to translate it into one of 99 languages.\n",
    "\n",
    "It's worth noting that the capability to translate into any language was discovered by accident during experiments with the model, and the official repository only states that it can translate any of the languages into English. The quality of recognition and translation may vary depending on the language being used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HPMZvbqD5KH4"
   },
   "source": [
    "## Setup\n",
    "This will install the necessary libraries and download the models. It may take a while, so please be patient."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "The dependencies have to be installed in a specific order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install -U yt-dlp\n",
    "!pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "!pip install pyannote.audio==2.1.1 denoiser==0.1.5 moviepy==1.0.3 pydub==0.25.1 git+https://github.com/openai/whisper.git@v20230124\n",
    "!pip install omegaconf==2.3.0 pytorch-lightning==1.8.4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start web server\n",
    "\n",
    "It will be used to serve HTML player, video file and subtitles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm install http-server -g\n",
    "\n",
    "import subprocess\n",
    "subprocess.Popen(['http-server', '-p', '8000']);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML player template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Template\n",
    "player_template = Template(\"\"\"\n",
    "<!doctype html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Speakers</title>\n",
    "<!-- <meta name=\"description\" content=\"Our first page\">\n",
    "<meta name=\"keywords\" content=\"html tutorial template\"> -->\n",
    "<style>\n",
    "    html, body, .video-container {\n",
    "        height: 100%;\n",
    "    }\n",
    "\n",
    "    .video-container {\n",
    "        background-color: #000;\n",
    "        display: flex;\n",
    "        flex-direction: column;\n",
    "    }\n",
    "\n",
    "    video {\n",
    "        max-height: 100%;\n",
    "    }\n",
    "\n",
    "    .speaker{\n",
    "        height: 100%;\n",
    "    }\n",
    "    .Background{\n",
    "        background-color: lightgray;\n",
    "    }    \n",
    "    /* Pastel pink */\n",
    "    .SPEAKER_00 {\n",
    "        background-color: #FEC8D8;\n",
    "    }\n",
    "\n",
    "    /* Pastel peach */\n",
    "    .SPEAKER_01 {\n",
    "        background-color: #ff8282;\n",
    "    }\n",
    "\n",
    "    /* Pastel yellow */\n",
    "    .SPEAKER_02 {\n",
    "        background-color: #acffb3;\n",
    "    }\n",
    "\n",
    "    /* Pastel green */\n",
    "    .SPEAKER_03 {\n",
    "        background-color: #C5E5C5;\n",
    "    }\n",
    "\n",
    "    /* Pastel blue */\n",
    "    .SPEAKER_04 {\n",
    "        background-color: #C3D9FF;\n",
    "    }\n",
    "\n",
    "    /* Pastel lavender */\n",
    "    .SPEAKER_05 {\n",
    "        background-color: #E9C7E9;\n",
    "    }\n",
    "\n",
    "    /* Pastel coral */\n",
    "    .SPEAKER_06 {\n",
    "        background-color: #FFD6D1;\n",
    "    }\n",
    "\n",
    "    /* Pastel salmon */\n",
    "    .SPEAKER_07{\n",
    "        background-color: #fffab4;\n",
    "    }\n",
    "\n",
    "    /* Pastel mint */\n",
    "    .SPEAKER_08 {\n",
    "        background-color: #D1E8D1;\n",
    "    }\n",
    "\n",
    "    /* Pastel sky blue */\n",
    "    .SPEAKER_09 {\n",
    "        background-color: #9b5f2b;\n",
    "    }\n",
    "\n",
    "    #video-controls{\n",
    "        background-color: rgba(0, 0, 0, 0.8);\n",
    "        padding: 1em;\n",
    "    }\n",
    "\n",
    "    #timeline{\n",
    "        margin-bottom: 1em;\n",
    "    }\n",
    "\n",
    "    #play {\n",
    "        border: 0;\n",
    "        background: transparent;\n",
    "        box-sizing: border-box;\n",
    "        width: 0;\n",
    "        height: 37px;\n",
    "        border-color: transparent transparent transparent #dbdbdb;\n",
    "        transition: 100ms all ease;\n",
    "        cursor: pointer;\n",
    "        border-style: solid;\n",
    "        border-width: 18px 0 18px 30px;\n",
    "        }\n",
    "    #play.paused {\n",
    "        border-style: double;\n",
    "        border-width: 0px 0 0px 30px;\n",
    "    }\n",
    "    #play:hover {\n",
    "        border-color: transparent transparent transparent #fff;\n",
    "    }\n",
    "    .slider{\n",
    "        -webkit-appearance: none;\n",
    "        width: 100%;\n",
    "        background: linear-gradient(to right, #ff7a7a 0%, #ff7a7a 0%, rgba(255, 255, 255, 0) 0%, rgba(255, 255, 255, 0) 100%);\n",
    "        outline: none;\n",
    "        transition: background 450ms ease-in;\n",
    "        opacity: 0.7;\n",
    "        position: absolute;\n",
    "        height: 10px;\n",
    "        margin: 0;\n",
    "    }\n",
    "\n",
    "    .slider:hover {\n",
    "        opacity: 1; /* Fully shown on mouse-over */\n",
    "    }\n",
    "\n",
    "    .buttons{\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "    }\n",
    "\n",
    "    #time{\n",
    "        margin-left: 1em;\n",
    "        color: #fff;\n",
    "    }\n",
    "    #speaker{\n",
    "        margin-left: 1em;\n",
    "        color: #fff;\n",
    "    }\n",
    "\n",
    "    @keyframes rotate {\n",
    "        from {\n",
    "            transform: rotate(0deg);\n",
    "        }\n",
    "        to { \n",
    "            transform: rotate(360deg);\n",
    "        }\n",
    "    }\n",
    " \n",
    "\n",
    "    @-webkit-keyframes rotate {\n",
    "        from {\n",
    "            -webkit-transform: rotate(0deg);\n",
    "        }\n",
    "        to { \n",
    "            -webkit-transform: rotate(360deg);\n",
    "        }\n",
    "    }\n",
    "\n",
    "    #load {\n",
    "        width: 20px;\n",
    "        height: 20px;\n",
    "        margin: 0 0 0;\n",
    "        border: solid 5px #fff;\n",
    "        border-radius: 50%;\n",
    "        border-right-color: transparent;\n",
    "        border-bottom-color: transparent;\n",
    "        -webkit-transition: all 0.5s ease-in;\n",
    "        -webkit-animation-name: rotate;\n",
    "        -webkit-animation-duration: 1.0s;\n",
    "        -webkit-animation-iteration-count: infinite;\n",
    "        -webkit-animation-timing-function: linear;\n",
    "        transition: all 0.5s ease-in;\n",
    "        animation-name: rotate;\n",
    "        animation-duration: 1.0s;\n",
    "        animation-iteration-count: infinite;\n",
    "        animation-timing-function: linear;\n",
    "    }\n",
    "    .hide{\n",
    "        display: none;\n",
    "    }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"video-container\">\n",
    "        <video controls class=\"video\" id=\"video\" preload=\"metadata\" crossorigin=\"use-credentials\">\n",
    "            <source src=\"$url\" type=\"video/mp4\"></source>\n",
    "            <track src=\"$vtt\" kind=\"subtitles\" srclang=\"en\" label=\"English\" default></track>\n",
    "        </video>\n",
    "        <div id=\"video-controls\">\n",
    "            <div style=\"position: relative;\">\n",
    "                <input type=\"range\" min=\"1\" max=\"100\" value=\"0\" step=\"0.05\" class=\"slider\" id=\"slider\">\n",
    "                <div id=\"timeline\" title='Background' style=\"display: flex; align-items: center; width: 100%; background-color: lightgray; height: 10px;\"></div>\n",
    "            </div>\n",
    "\n",
    "            <div class=\"buttons\">\n",
    "                <div id=\"load\"></div>\n",
    "                <div id=\"play\" title=\"Play\" class=\"hide\"></div>\n",
    "                <div id=\"time\">00:00 / 00:00</div>\n",
    "                <div id=\"speaker\"></div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "<script type=\"text/javascript\">\n",
    "    const zeroPad = (num, places=2) => String(num).padStart(places, '0')\n",
    "\n",
    "    let seeking = false;\n",
    "\n",
    "    slider = document.getElementById('slider');\n",
    "    slider.addEventListener(\"mousedown\", (event) => {\n",
    "        seeking = true;\n",
    "    });\n",
    "    slider.addEventListener(\"mouseup\", (event) => {\n",
    "        seeking = false;\n",
    "    });\n",
    "    slider.addEventListener(\"input\", (event) => {\n",
    "        let value = event.target.value\n",
    "        event.target.style.background = 'linear-gradient(to right, #ff7a7a 0%, #ff7a7a ' + value + '%, rgba(255, 255, 255, 0) ' + value + '%, rgba(255, 255, 255, 0) 100%)'\n",
    "    });\n",
    "    slider.addEventListener(\"change\", (event) => {\n",
    "        video.currentTime = (event.target.value / 100) * video.duration\n",
    "    });\n",
    "\n",
    "    const time = document.getElementById('time');\n",
    "\n",
    "    const video = document.getElementById('video');\n",
    "    video.controls = false;\n",
    "    video.addEventListener('timeupdate', (event) => {\n",
    "        if (!seeking) {\n",
    "            let value = (video.currentTime / video.duration) * 100\n",
    "            slider.style.background = 'linear-gradient(to right, #ff7a7a 0%, #ff7a7a ' + value + '%, rgba(255, 255, 255, 0) ' + value + '%, rgba(255, 255, 255, 0) 100%)'\n",
    "            slider.value = value;\n",
    "            }\n",
    "\n",
    "            let minutes = Math.floor(video.currentTime / 60);\n",
    "            let seconds = Math.floor(video.currentTime - minutes * 60);\n",
    "            let total_minutes = Math.floor(video.duration / 60);\n",
    "            let total_seconds = Math.floor(video.duration - total_minutes * 60);\n",
    "            time.innerHTML = `${zeroPad(minutes)}:${zeroPad(seconds)} / ${zeroPad(total_minutes)}:${zeroPad(total_seconds)}`;\n",
    "    });\n",
    "    \n",
    "    const speaker = document.getElementById('speaker');\n",
    "    const track = video.textTracks[0];\n",
    "    track.addEventListener('cuechange', () => {\n",
    "        const cues = track.activeCues[0];\n",
    "        if(!cues)\n",
    "            return;\n",
    "        const speaker_match = cues.text.match(/<v.(.*?)>/);\n",
    "            if (speaker_match)\n",
    "                speaker.innerHTML = speaker_match[1];  \n",
    "    });\n",
    "\n",
    "    const load = document.getElementById('load');\n",
    "    const play = document.getElementById('play');\n",
    "\n",
    "    let video_loaded = false;\n",
    "\n",
    "    video.addEventListener('loadedmetadata', (event) => {\n",
    "        video_loaded = true;\n",
    "        load.classList.add('hide');\n",
    "        play.classList.remove('hide');\n",
    "        let total_minutes = Math.floor(video.duration / 60);\n",
    "        let total_seconds = Math.floor(video.duration - total_minutes * 60);\n",
    "        time.innerHTML = `00:00 / ${zeroPad(total_minutes)}:${zeroPad(total_seconds)}`;\n",
    "    });\n",
    "    \n",
    "    const videoControls = document.getElementById('video-controls');\n",
    "    const timeline = document.getElementById('timeline');\n",
    "\n",
    "    video.addEventListener('click', () => {\n",
    "        if (!video_loaded)\n",
    "            return;\n",
    "        if (video.paused) {\n",
    "            video.play();\n",
    "        }\n",
    "        else {\n",
    "            video.pause();\n",
    "        }\n",
    "    });\n",
    "\n",
    "    play.addEventListener('click', () => {\n",
    "        if (!video_loaded)\n",
    "            return;\n",
    "        if (video.paused) {\n",
    "            video.play();\n",
    "        } else {\n",
    "            video.pause();\n",
    "        }\n",
    "    });\n",
    "\n",
    "    video.addEventListener('play', () => {\n",
    "        play.classList.add('paused');\n",
    "    });\n",
    "\n",
    "    video.addEventListener('pause', () => {\n",
    "        play.classList.remove('paused');\n",
    "    });\n",
    "\n",
    "\n",
    "    const percentages = $percentages;\n",
    "\n",
    "    \n",
    "\n",
    "    async function main(){\n",
    "        let divs = ''\n",
    "        let last_time = 0\n",
    "        for(let p of percentages){\n",
    "            divs += `<div class=\"speaker ${p[0]}\" style=\"width:${p[1]}%;\" title=\"${p[0]}\"></div>\\n`\n",
    "        }\n",
    "        timeline.innerHTML = divs\n",
    "    }\n",
    "\n",
    "    main()\n",
    "</script>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "import tempfile\n",
    "from os.path import join as opj\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "\n",
    "import whisper\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "from denoiser.audio import Audioset\n",
    "from denoiser import distrib, pretrained\n",
    "from denoiser.audio import Audioset, find_audio_files\n",
    "\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "from moviepy.editor import AudioFileClip, concatenate_audioclips\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "denoise_model = pretrained.get_model(Namespace(model_path=None, dns48=False, dns64=False, master64=False, valentini_nc=False)).to(device)\n",
    "denoise_model.eval()\n",
    "whisper_model = whisper.load_model(\"large\").to(device)\n",
    "whisper_model.eval()\n",
    "\n",
    "def split_audio(tmpdirname, video, chunk_size=120):\n",
    "    \"\"\"\n",
    "    Split audio into chunks of chunk_size\n",
    "    \"\"\"\n",
    "    path = opj(tmpdirname, 'noisy_chunks')\n",
    "    os.makedirs(path)\n",
    "    # extract audio from video\n",
    "    audio = AudioFileClip(video.name)\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as audio_fp:\n",
    "        audio.write_audiofile(audio_fp.name, verbose=False)\n",
    "\n",
    "        # round duration to the next whole integer\n",
    "        for i, chunk in enumerate(np.arange(0, audio.duration, chunk_size)):\n",
    "            ffmpeg_extract_subclip(audio_fp.name, chunk, min(chunk + chunk_size, audio.duration),\n",
    "                                targetname=opj(path, f'{i:09}.wav'))\n",
    "    return audio.duration\n",
    "\n",
    "\n",
    "def get_speakers(tmpdirname, use_auth_token=True):\n",
    "    files = find_audio_files(opj(tmpdirname, 'noisy_chunks'))\n",
    "    dset = Audioset(files, with_path=True,\n",
    "                    sample_rate=denoise_model.sample_rate, channels=denoise_model.chin, convert=True)\n",
    "    \n",
    "    loader = distrib.loader(dset, batch_size=1)\n",
    "    distrib.barrier()\n",
    "\n",
    "    print('removing noise...')\n",
    "    enhanced_chunks = []\n",
    "    with tempfile.TemporaryDirectory() as denoised_tmpdirname:\n",
    "        for data in loader:\n",
    "            noisy_signals, filenames = data\n",
    "            noisy_signals = noisy_signals.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                wav = denoise_model(noisy_signals).squeeze(0)\n",
    "            wav = wav / max(wav.abs().max().item(), 1)\n",
    "\n",
    "            name = opj(denoised_tmpdirname, filenames[0].split('/')[-1])\n",
    "            torchaudio.save(name, wav.cpu(), denoise_model.sample_rate)\n",
    "            enhanced_chunks.append(name)\n",
    "\n",
    "        print('reassembling chunks...')\n",
    "        clips = [AudioFileClip(c) for c in sorted(enhanced_chunks)]\n",
    "        final_clip = concatenate_audioclips(clips)\n",
    "        cleaned_path = opj(tmpdirname, 'cleaned.wav')\n",
    "        final_clip.write_audiofile(cleaned_path, verbose=False)\n",
    "        \n",
    "        print('identifying speakers...')\n",
    "        # load pre-trained model\n",
    "        pipeline = Pipeline.from_pretrained('pyannote/speaker-diarization', use_auth_token=use_auth_token)\n",
    "    \n",
    "        return str(pipeline({'uri': '', 'audio': cleaned_path})).split('\\n'), cleaned_path\n",
    "\n",
    "def get_subtitles(timecodes, clened_audio_path, language=None):\n",
    "    if(device == 'cpu'):\n",
    "        options = whisper.DecodingOptions(language=language, fp16=False)\n",
    "    else:\n",
    "        options = whisper.DecodingOptions(language=language)\n",
    "\n",
    "    timeline = {}\n",
    "    prev_speaker = None\n",
    "    prev_start = 0\n",
    "    for line in timecodes:\n",
    "        start, end = re.findall(r'\\d{2}:\\d{2}:\\d{2}.\\d{3}', line)\n",
    "        start = str_to_seconds(start)\n",
    "        end = str_to_seconds(end)\n",
    "        speaker = re.findall(r'\\w+$', line)[0]\n",
    "\n",
    "        # extract a segment of the audio for a speaker\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=True) as audio_fp:\n",
    "            ffmpeg_extract_subclip(clened_audio_path, start, end,\n",
    "                                    targetname=audio_fp.name)\n",
    "\n",
    "            # load audio and pad/trim it to fit 30 seconds\n",
    "            audio = whisper.load_audio(audio_fp.name)\n",
    "            audio = whisper.pad_or_trim(audio)  \n",
    "            # make log-Mel spectrogram and move to the same device as the model\n",
    "            mel = whisper.log_mel_spectrogram(audio).to(whisper_model.device)\n",
    "            # decode the audio\n",
    "            result = whisper.decode(whisper_model, mel, options)\n",
    "\n",
    "            if(speaker == prev_speaker):\n",
    "                timeline[prev_start]['text'] += f' <{seconds_to_str(start)}>{result.text}'\n",
    "                timeline[prev_start]['end'] = end\n",
    "            else:\n",
    "                timeline[start] = { 'end': end, \n",
    "                                    'speaker': speaker,\n",
    "                                    'text': f'<v.{speaker}>{speaker}</v>: {result.text}'}\n",
    "                prev_start = start\n",
    "\n",
    "            prev_speaker = speaker\n",
    "\n",
    "    return timeline\n",
    "\n",
    "def str_to_seconds(time_str):\n",
    "    h, m, s = time_str.split(':')\n",
    "    return int(h) * 3600 + int(m) * 60 + float(s)\n",
    "\n",
    "def seconds_to_str(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    # with milliseconds\n",
    "    return f'{int(h):02}:{int(m):02}:{s:06.3f}'\n",
    "    \n",
    "\n",
    "def timeline_to_vtt(timeline):\n",
    "    vtt = 'WEBVTT\\n\\n'\n",
    "    for start in sorted(timeline.keys()):\n",
    "        end = timeline[start]['end']\n",
    "        text = timeline[start]['text']\n",
    "        vtt += f'{seconds_to_str(start)} --> {seconds_to_str(end)}\\n'\n",
    "        vtt += text+'\\n\\n'\n",
    "    return vtt\n",
    "\n",
    "def calc_speaker_percentage(timeline, duration):\n",
    "    percentages = []\n",
    "    end = 0\n",
    "    for start in sorted(timeline.keys()):\n",
    "        if(start > end):\n",
    "            percentages.append(['Background', 100*(start-end)/duration])\n",
    "        end = timeline[start]['end']\n",
    "        speaker = timeline[start]['speaker']\n",
    "        percentages.append([speaker, 100*(end-start)/duration])\n",
    "    return percentages\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UI code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zQrxfIoNlTI"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get get an external URL to the virtual machine\n",
    "from google.colab.output import eval_js\n",
    "url = eval_js(\"google.colab.kernel.proxyPort(8000)\")\n",
    "\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "# List of supported languages\n",
    "from whisper.tokenizer import LANGUAGES\n",
    "\n",
    "# workaround\n",
    "# https://github.com/pyannote/pyannote-audio/issues/1269\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display, HTML, Markdown\n",
    "import tempfile\n",
    "\n",
    "def render_player():\n",
    "  return HTML(player_template.safe_substitute(url='http://localhost:8000/video.mp4', vtt='http://localhost:8000/subtitles.vtt', percentages=str(percentages)))\n",
    "\n",
    "# Input form elements\n",
    "out = widgets.Output()\n",
    "upload = widgets.FileUpload(accept='.mp4', button_style='info')\n",
    "text = widgets.Text(placeholder='Youtube URL')\n",
    "lang_options = [('Original', None)]+[(v.capitalize(), k) for k, v in LANGUAGES.items()]\n",
    "lang = widgets.Dropdown(options=lang_options, description='Translate to:')\n",
    "\n",
    "percentages = []\n",
    "\n",
    "def process():  \n",
    "  global percentages\n",
    "  print('Processing...')\n",
    "\n",
    "  # remove old file if exists\n",
    "  !rm -f video.mp4\n",
    "  if text.value: # if URL to video is provided\n",
    "    with YoutubeDL({'format': 'bv[ext=mp4]+bv[height<=1080]+ba[ext=m4a]', 'outtmpl': 'video.mp4'}) as ydl:\n",
    "      ydl.download([text.value])\n",
    "  else: # if file is uploaded\n",
    "    with open('video.mp4', 'wb') as f:\n",
    "      uploaded_filename = next(iter(upload.value))\n",
    "      content = upload.value[uploaded_filename]['content']\n",
    "      f.write(content)\n",
    "  \n",
    "  clear_output()\n",
    "        \n",
    "  with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    # split audio to fit in memory\n",
    "    with open('video.mp4', 'rb') as f:\n",
    "      duration = split_audio(tmpdirname, f)\n",
    "\n",
    "    speaker_diarisation, cleaned_path = get_speakers(tmpdirname)\n",
    "\n",
    "    clear_output()\n",
    "\n",
    "    print('Language', lang.value)\n",
    "    timeline  = get_subtitles(speaker_diarisation, cleaned_path, language=lang.value)\n",
    "\n",
    "    vtt = timeline_to_vtt(timeline).encode('utf-8')\n",
    "    percentages = calc_speaker_percentage(timeline, duration)\n",
    "    with open('subtitles.vtt', 'wb') as f:\n",
    "      f.write(vtt)\n",
    "\n",
    "    clear_output()\n",
    "    \n",
    "    with open('index.html', 'w') as f:\n",
    "      f.write(player_template.safe_substitute(url=f'{url}/video.mp4', vtt=f'{url}/subtitles.vtt', percentages=str(percentages)))\n",
    "    \n",
    "    # print markdown link to the video\n",
    "    display(Markdown(f'Open [player]({url}/index.html) in a new tab or [download subtitles]({url}/subtitles.vtt)'))\n",
    "\n",
    "\n",
    "def render_upload_form():\n",
    "  return widgets.VBox([text, widgets.Label(value='or'), upload, lang])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWo0u88o5Vdl"
   },
   "source": [
    "## Login on huggingface\n",
    "\n",
    "**Note: You will need to accept pyannote's [speaker-diarization](https://huggingface.co/pyannote/speaker-diarization) and [segmentation](https://huggingface.co/pyannote/segmentation) user conditions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VI8rcG7se0f"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b23tMUFV5rn6"
   },
   "source": [
    "## UI\n",
    "\n",
    "You can either paste a YouTube link or upload a video file. If a link is provided, upload will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ftqEhHH0YNwB"
   },
   "outputs": [],
   "source": [
    "render_upload_form()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qgbwvk-7FqV"
   },
   "outputs": [],
   "source": [
    "process()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: To be able to download subtitles open player in a new tab using the links above, 3rd party cookies must be enabled.**\n",
    "\n",
    "Otherwise, you can download the subtitles from the file explorer. You can also render the player in the notebook by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNFJQAof7G4O"
   },
   "outputs": [],
   "source": [
    "render_player()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "HPMZvbqD5KH4"
   ],
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "376849e4ad5f5f9e6c30ad49542ed8906757d0bad303a7288fc2c6ecf3f60a81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
